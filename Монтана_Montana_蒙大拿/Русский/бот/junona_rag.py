# junona_rag.py
# –Æ–Ω–æ–Ω–∞ –∑–Ω–∞–µ—Ç –í–°–Å ‚Äî RAG-—Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–æ–ª–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π Montana

import os
import hashlib
import json
from pathlib import Path
from typing import List, Dict, Optional
from dataclasses import dataclass

# –ü—É—Ç–∏
MONTANA_ROOT = Path(__file__).parent.parent.parent  # –ú–æ–Ω—Ç–∞–Ω–∞_Montana_ËíôÂ§ßÊãø
ACP_ROOT = Path("/Users/kh./Python/ACP_1/Montana ACP")
BOT_DIR = Path(__file__).parent
INDEX_DIR = BOT_DIR / "data" / "rag_index"
METADATA_FILE = INDEX_DIR / "file_hashes.json"

# –†–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
INDEXABLE_EXTENSIONS = {".md", ".py", ".txt", ".rs", ".toml"}

# –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ (—Å–∏–º–≤–æ–ª–æ–≤)
CHUNK_SIZE = 1500
CHUNK_OVERLAP = 200


@dataclass
class Document:
    content: str
    metadata: Dict
    embedding: Optional[List[float]] = None


class JunonaRAG:
    def __init__(self):
        self.index_dir = INDEX_DIR
        self.index_dir.mkdir(parents=True, exist_ok=True)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã
        self.collection = None
        self.embedder = None
        self._init_components()

    def _init_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB –∏ embeddings"""
        try:
            import chromadb
            from chromadb.config import Settings

            # –õ–æ–∫–∞–ª—å–Ω–∞—è –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–∞—è –±–∞–∑–∞
            self.client = chromadb.PersistentClient(
                path=str(self.index_dir / "chroma"),
                settings=Settings(anonymized_telemetry=False)
            )

            # –ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–ª—è Montana
            self.collection = self.client.get_or_create_collection(
                name="montana_knowledge",
                metadata={"description": "–ü–æ–ª–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π Montana"}
            )

            # Embeddings ‚Äî sentence-transformers (–ª–æ–∫–∞–ª—å–Ω–æ, –±–µ—Å–ø–ª–∞—Ç–Ω–æ)
            from sentence_transformers import SentenceTransformer
            self.embedder = SentenceTransformer('intfloat/multilingual-e5-small')

            print(f"üß† –Æ–Ω–æ–Ω–∞ RAG: ChromaDB + multilingual-e5-small")
            print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ: {self.collection.count()}")

        except ImportError as e:
            print(f"‚ö†Ô∏è RAG –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install chromadb sentence-transformers")

    def _get_file_hash(self, filepath: Path) -> str:
        """SHA256 —Ö–µ—à —Ñ–∞–π–ª–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        content = filepath.read_bytes()
        return hashlib.sha256(content).hexdigest()[:16]

    def _load_metadata(self) -> Dict[str, str]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ö–µ—à–∏ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        if METADATA_FILE.exists():
            return json.loads(METADATA_FILE.read_text())
        return {}

    def _save_metadata(self, metadata: Dict[str, str]):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ö–µ—à–∏"""
        METADATA_FILE.write_text(json.dumps(metadata, indent=2))

    def _chunk_text(self, text: str, filepath: str) -> List[Document]:
        """–†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º"""
        chunks = []

        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º —Å–Ω–∞—á–∞–ª–∞
        paragraphs = text.split('\n\n')
        current_chunk = ""

        for para in paragraphs:
            if len(current_chunk) + len(para) < CHUNK_SIZE:
                current_chunk += para + "\n\n"
            else:
                if current_chunk.strip():
                    chunks.append(Document(
                        content=current_chunk.strip(),
                        metadata={
                            "source": filepath,
                            "type": Path(filepath).suffix,
                            "lang": self._detect_lang(filepath)
                        }
                    ))
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
                overlap = current_chunk[-CHUNK_OVERLAP:] if len(current_chunk) > CHUNK_OVERLAP else ""
                current_chunk = overlap + para + "\n\n"

        # –ü–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk.strip():
            chunks.append(Document(
                content=current_chunk.strip(),
                metadata={
                    "source": filepath,
                    "type": Path(filepath).suffix,
                    "lang": self._detect_lang(filepath)
                }
            ))

        return chunks

    def _detect_lang(self, filepath: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫ –ø–æ –ø—É—Ç–∏"""
        fp = filepath.lower()
        if "—Ä—É—Å—Å–∫–∏–π" in fp or "russian" in fp:
            return "ru"
        elif "english" in fp:
            return "en"
        elif "‰∏≠Êñá" in fp or "chinese" in fp:
            return "zh"
        return "ru"  # default

    def _collect_files(self) -> List[Path]:
        """–°–æ–±—Ä–∞—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        files = []

        # Montana –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
        for ext in INDEXABLE_EXTENSIONS:
            files.extend(MONTANA_ROOT.rglob(f"*{ext}"))

        # ACP –ø—Ä–æ—Ç–æ–∫–æ–ª (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if ACP_ROOT.exists():
            for ext in INDEXABLE_EXTENSIONS:
                files.extend(ACP_ROOT.rglob(f"*{ext}"))

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Ñ–∞–π–ª—ã
        files = [f for f in files if not any(p in str(f) for p in [
            "__pycache__", ".git", "node_modules", "venv", ".env"
        ])]

        return files

    def index(self, force: bool = False):
        """–ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã Montana"""
        if not self.collection or not self.embedder:
            print("‚ö†Ô∏è RAG –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return

        files = self._collect_files()
        metadata = self._load_metadata()

        new_docs = 0
        updated_docs = 0

        for filepath in files:
            try:
                file_hash = self._get_file_hash(filepath)
                filepath_str = str(filepath)

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è
                if not force and metadata.get(filepath_str) == file_hash:
                    continue

                # –ß–∏—Ç–∞–µ–º –∏ —á–∞–Ω–∫—É–µ–º
                try:
                    content = filepath.read_text(encoding='utf-8')
                except UnicodeDecodeError:
                    continue  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ —Ñ–∞–π–ª—ã

                if len(content) < 50:
                    continue  # —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ

                chunks = self._chunk_text(content, filepath_str)

                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏ —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
                try:
                    self.collection.delete(where={"source": filepath_str})
                except:
                    pass

                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ
                for i, chunk in enumerate(chunks):
                    doc_id = f"{file_hash}_{i}"
                    embedding = self.embedder.encode(chunk.content).tolist()

                    self.collection.add(
                        ids=[doc_id],
                        embeddings=[embedding],
                        documents=[chunk.content],
                        metadatas=[chunk.metadata]
                    )

                # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                if filepath_str in metadata:
                    updated_docs += 1
                else:
                    new_docs += 1
                metadata[filepath_str] = file_hash

            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ {filepath}: {e}")

        self._save_metadata(metadata)

        print(f"‚úì –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: +{new_docs} –Ω–æ–≤—ã—Ö, ~{updated_docs} –æ–±–Ω–æ–≤–ª–µ–Ω–æ")
        print(f"  –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {self.collection.count()} —á–∞–Ω–∫–æ–≤")

    def search(self, query: str, n_results: int = 5, lang: str = None) -> List[Dict]:
        """–ù–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
        if not self.collection or not self.embedder:
            return []

        # Embedding –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.embedder.encode(query).tolist()

        # –§–∏–ª—å—Ç—Ä –ø–æ —è–∑—ã–∫—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        where_filter = {"lang": lang} if lang else None

        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results,
            where=where_filter,
            include=["documents", "metadatas", "distances"]
        )

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        docs = []
        for i in range(len(results['documents'][0])):
            docs.append({
                "content": results['documents'][0][i],
                "source": results['metadatas'][0][i]['source'],
                "lang": results['metadatas'][0][i]['lang'],
                "score": 1 - results['distances'][0][i]  # similarity score
            })

        return docs

    def get_context(self, query: str, max_tokens: int = 2000) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM –∏–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        docs = self.search(query, n_results=5)

        if not docs:
            return ""

        context_parts = []
        current_tokens = 0  # –≥—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞

        for doc in docs:
            # ~4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
            doc_tokens = len(doc['content']) // 4

            if current_tokens + doc_tokens > max_tokens:
                break

            source_name = Path(doc['source']).stem
            context_parts.append(f"[{source_name}]\n{doc['content']}")
            current_tokens += doc_tokens

        return "\n\n---\n\n".join(context_parts)


# Singleton instance
_rag = None

def get_rag() -> JunonaRAG:
    global _rag
    if _rag is None:
        _rag = JunonaRAG()
    return _rag


def init_and_index(background: bool = True):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å RAG –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é"""
    rag = get_rag()

    if background:
        import threading
        def _index():
            rag.index(force=False)
        thread = threading.Thread(target=_index, daemon=True)
        thread.start()
        print("üîÑ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–ø—É—â–µ–Ω–∞ –≤ —Ñ–æ–Ω–µ")
    else:
        rag.index(force=False)

    return rag


def reindex_if_needed():
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (–¥–ª—è watchdog)"""
    rag = get_rag()
    rag.index(force=False)


# CLI –¥–ª—è –ø–µ—Ä–≤–∏—á–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
if __name__ == "__main__":
    import sys

    rag = get_rag()

    if len(sys.argv) > 1 and sys.argv[1] == "--force":
        print("üîÑ –ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è...")
        rag.index(force=True)
    else:
        print("üîÑ –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è...")
        rag.index(force=False)

    # –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞
    print("\nüìñ –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞: '—á—Ç–æ —Ç–∞–∫–æ–µ ACP'")
    results = rag.search("—á—Ç–æ —Ç–∞–∫–æ–µ ACP", n_results=3)
    for r in results:
        print(f"  [{r['score']:.2f}] {Path(r['source']).name}")
        print(f"       {r['content'][:100]}...")
